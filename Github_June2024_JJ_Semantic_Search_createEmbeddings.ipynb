{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justinjunge/Convergent-Wisdom-Project/blob/main/Github_June2024_JJ_Semantic_Search_createEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original goal of this notebook was to semantically compare any arbitrary sentence to every sentence in the Bible and print the top #X matches.\n",
        "\n",
        "Jan 13, 2023: This version works for the wikipedia entry on Buddhism\n",
        "\n",
        "Jan 15, 2023: This version now works for the entire Christian Bible, and generates a model embedding that can be exported and reused for efficiency. (With default settings it takes about 45 minutes to embed).   \n",
        "\n",
        "Feb 2, 2023: This version was modified to generate a model embedding of the Bhagavad Gita, and is now being modified for the Quran\n",
        "\n",
        "June 8, 2024: Returning to incomplete embedding of Quran. Note: completed.\n",
        "\n",
        "June 20: Returning to embedding of Swedenborg's Writings\n"
      ],
      "metadata": {
        "id": "oaFeF1rCkwW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# placed at top for manual steps\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "10NEoh5N30dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGCtyvb0kvGf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy import stats\n",
        "import random\n",
        "import copy\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "import torch\n",
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "!pip install matplotlib-venn\n",
        "from matplotlib_venn import venn2, venn2_circles, venn3, venn3_circles"
      ],
      "metadata": {
        "id": "b6h1-jg-lIRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "gHV0ZU44oR6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = SentenceTransformer('stsb-roberta-large')\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "fNFQgdQLlZlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test zone\n",
        "sentence1 = \"William James started the modern field of Psychology as both an empirical and theoretical domain. \"\n",
        "sentence2 = \"Skittles are a snack food that is unhealthy.\"\n",
        "\n",
        "# encode sentences to get their embeddings\n",
        "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "print(\"Sentence 1:\", sentence1)\n",
        "print(\"Sentence 2:\", sentence2)\n",
        "print(\"Similarity score:\", cosine_scores.item())"
      ],
      "metadata": {
        "id": "DQNKzh1ImuCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"There are in general four differing styles in the Word.\"]\n",
        "body_text = [\"I was told from heaven that there are four special styles of the old Testament.\", \"The [first is the] style of the ancient Church and of the most ancient, which is like that of the Book of Genesis treating of Paradise, and the tower of Babel.\"]\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "embedding2 = model.encode(body_text, convert_to_tensor=True)\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "for i in range(len(input_phrase)):\n",
        "    for j in range(len(body_text)):\n",
        "        print(\"Sentence 1:\", input_phrase[i])\n",
        "        print(\"Sentence 2:\", body_text[j])\n",
        "        print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "        print()"
      ],
      "metadata": {
        "id": "4EajMczRnHA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = wikipedia.summary(\"Buddhism\")\n",
        "token_text1 = sent_tokenize(text1)\n",
        "\n",
        "text2 = wikipedia.summary(\"Hinduism\")\n",
        "token_text2 = sent_tokenize(text2)\n",
        "\n",
        "sentences1 = token_text1\n",
        "sentences2 = token_text2\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(sentences1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(sentences2, convert_to_tensor=True)\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "for i in range(len(sentences1)):\n",
        "    for j in range(len(sentences2)):\n",
        "        if cosine_scores[i][j].item() > 0.6:\n",
        "          print(\"Sentence 1:\", sentences1[i])\n",
        "          print(\"Sentence 2:\", sentences2[j])\n",
        "          print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "          print()"
      ],
      "metadata": {
        "id": "rjz1c90DoNI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"Meditation is a way to connect more deeply to reality.\"]\n",
        "body_text = wikipedia.summary(\"Buddhism\")\n",
        "body_token = sent_tokenize(body_text)\n",
        "\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "embedding2 = model.encode(body_token, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])\n",
        "\n",
        "#for i in range(len(input_phrase)):\n",
        "    #for j in range(len(body_token)):\n",
        "      #if cosine_scores[i][j].item() > 0.6:\n",
        "        #print(\"Sentence 1:\", input_phrase[i])\n",
        "        #print(\"Sentence 2:\", body_text[j])\n",
        "        #print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "        #print()\n",
        "\n"
      ],
      "metadata": {
        "id": "H8Nft51UoxHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"Meditation is a way to connect more deeply to reality.\"]\n",
        "body_text = wikipedia.page(\"Buddhism\").content\n",
        "body_token = sent_tokenize(body_text)\n",
        "\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "embedding2 = model.encode(body_token, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])\n",
        "\n",
        "#for i in range(len(input_phrase)):\n",
        "    #for j in range(len(body_token)):\n",
        "      #if cosine_scores[i][j].item() > 0.6:\n",
        "        #print(\"Sentence 1:\", input_phrase[i])\n",
        "        #print(\"Sentence 2:\", body_text[j])\n",
        "        #print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "        #print()"
      ],
      "metadata": {
        "id": "NYz1rfjBqI0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_list = list(cosine_scores[0])\n",
        "#a_list = [10, 11, 14, 23, 9, 3, 35, 22]\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value)\n",
        "print(max_index)"
      ],
      "metadata": {
        "id": "NGdNgmpYrf4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_list[15]"
      ],
      "metadata": {
        "id": "UjmJWLdIsY8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df3 = pd.read_csv ('/content/gdrive/MyDrive/jj_Quran.csv')\n",
        "df3 = pd.read_csv ('/content/gdrive/MyDrive/ncbsw_3a.csv')\n",
        "print(df3)"
      ],
      "metadata": {
        "id": "zyNVx9jKG01S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Writings2 = df3\n"
      ],
      "metadata": {
        "id": "fRU284GYHji1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a new dataframe that contains one entry for each unique chapter appending all of the text in \"text\" column into one cell per uniquechap\n",
        "\n",
        "df_Writings_chapter2 = df_Writings2.groupby('uniquechap')['text'].apply(lambda x: ' '.join(x)).to_frame().reset_index()\n"
      ],
      "metadata": {
        "id": "LJaWfTTIKOS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Writings_chapter2"
      ],
      "metadata": {
        "id": "8_1NQFkpHN1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Quran_chapter[\"Text\"][0]"
      ],
      "metadata": {
        "id": "5Rs4U5WaadV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the column of interest\n",
        "column = df_Writings_chapter2[\"text\"]\n",
        "\n",
        "# Convert the column to a list of strings with quotes\n",
        "quoted_list = ['\"' + str(x) + '\"' for x in column]"
      ],
      "metadata": {
        "id": "MQ-BavpLVdIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Writings_Embedding2 = model.encode(quoted_list, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "g7RbqaRuLl1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"What is the best thing that a person can do?\"]\n",
        "#body_text = wikipedia.page(\"Buddhism\").content\n",
        "#body_token = sent_tokenize(body_text)\n",
        "#body_token = quoted_list[0:1000]\n",
        "# encode list of sentences to get their embeddings\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "#embedding2 = model.encode(body_token, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])\n",
        "\n",
        "#for i in range(len(input_phrase)):\n",
        "    #for j in range(len(body_token)):\n",
        "      #if cosine_scores[i][j].item() > 0.6:\n",
        "        #print(\"Sentence 1:\", input_phrase[i])\n",
        "        #print(\"Sentence 2:\", body_text[j])\n",
        "        #print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
        "        #print()"
      ],
      "metadata": {
        "id": "6jD2yc1eJzBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(unique_list)"
      ],
      "metadata": {
        "id": "GlE5NcS3KqdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_phrase = [\"What is the meaning of life?\"]\n",
        "body_token = unique_list\n",
        "# encode list of sentences to get their embeddings\n",
        "# embedding1 = model.encode(input_phrase, convert_to_tensor=True)  # this is the time intensive step of the entire process\n",
        "Writings_chapter2 = model.encode(body_token, convert_to_tensor=True)\n",
        "#print(embeddingQc)"
      ],
      "metadata": {
        "id": "IV3_L1TW3o99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Writings[0])"
      ],
      "metadata": {
        "id": "ZyeAbp753tj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"Live a good and useful life.\"]\n",
        "\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, Writings)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])"
      ],
      "metadata": {
        "id": "7BcNAO5jD3kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"Where does God live?\"]\n",
        "\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embeddingQc)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])"
      ],
      "metadata": {
        "id": "UPRz0MexRI_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"How should we think about hate?\"]\n",
        "\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])"
      ],
      "metadata": {
        "id": "1ILwmRlDRUYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"How should we think about love and enemies?\"]\n",
        "\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])"
      ],
      "metadata": {
        "id": "cgVOmPEyR079"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = [\"Don't be afraid\"]\n",
        "\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "# compute similarity scores of two embeddings\n",
        "cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "\n",
        "score_list = list(cosine_scores[0])\n",
        "\n",
        "max_value = max(score_list)\n",
        "max_index = score_list.index(max_value)\n",
        "\n",
        "print(max_value, input_phrase, body_token[max_index])"
      ],
      "metadata": {
        "id": "8FFmK_4ylyna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch"
      ],
      "metadata": {
        "id": "7dk9wdIzNmRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddingBc = embedding2"
      ],
      "metadata": {
        "id": "AeTluBlV-Q88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(Writings_Embedding2, \"Writings_Embedding2.pt\")"
      ],
      "metadata": {
        "id": "xWtySjbmK5bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "VU0iApsRPEYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('Writings_Embedding2.pt')"
      ],
      "metadata": {
        "id": "T9EPV96kOthO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below this includes examples of loading the .csv files with the texts, along with loading the embeddings."
      ],
      "metadata": {
        "id": "H92TPP8eNYmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below will load previously saved embeddings (transformed to vectors), but needs access to the file AND needs a GPU runtime\n",
        "\n",
        "# embeddingB = torch.load('/content/gdrive/MyDrive/Bible_Embedding.pt', map_location='cuda')"
      ],
      "metadata": {
        "id": "AUYlRUisK68z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddingG = torch.load('/content/gdrive/MyDrive/Gita3_Embedding.pt', map_location='cuda')\n",
        "embeddingB = torch.load('/content/gdrive/MyDrive/Bible_Embedding.pt', map_location='cuda')\n",
        "embeddingQ = torch.load('/content/gdrive/MyDrive/Quran_Embedding.pt', map_location='cuda')"
      ],
      "metadata": {
        "id": "cvLmHiZ7N2od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Gita = pd.read_csv ('/content/gdrive/MyDrive/jj_SrimadBhagavadGita.csv')\n",
        "df_Bible = pd.read_csv ('/content/gdrive/MyDrive/jj_ChristianBible.csv')\n",
        "df_Quran = pd.read_csv ('/content/gdrive/MyDrive/jj_Quran.csv')\n",
        "#print(df3)\n",
        "# Read in the data\n",
        "\n",
        "# Get the column of interest\n",
        "columnG = df_Gita[\"Text\"]\n",
        "columnB = df_Bible[\"text\"]\n",
        "columnQ = df_Quran[\"Text\"]\n",
        "\n",
        "# Convert the column to a list of strings with quotes\n",
        "quoted_listG = ['\"' + str(x) + '\"' for x in columnG]\n",
        "quoted_listB = ['\"' + str(x) + '\"' for x in columnB]\n",
        "quoted_listQ = ['\"' + str(x) + '\"' for x in columnQ]"
      ],
      "metadata": {
        "id": "dOg-t0UUNXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns top N [num_results] matches to a phrase from the Gita, Bible, & Quran\n",
        "\n",
        "input_phrase = [\"Love everyone and be kind to everyone.\"]\n",
        "num_results = 2\n",
        "embedding1 = model.encode(input_phrase, convert_to_tensor=True)\n",
        "\n",
        "\n",
        "# compute similarity scores Gita, Bible, Quran\n",
        "# Gita\n",
        "cosine_scoresG = util.pytorch_cos_sim(embedding1, embeddingG)\n",
        "score_listG = list(cosine_scoresG[0])\n",
        "sorted_listG = sorted(score_listG)\n",
        "top_NG = sorted_listG[-num_results:]\n",
        "top_NG = sorted(top_NG, reverse=True)\n",
        "# Bible\n",
        "cosine_scoresB = util.pytorch_cos_sim(embedding1, embeddingB)\n",
        "score_listB = list(cosine_scoresB[0])\n",
        "sorted_listB = sorted(score_listB)\n",
        "top_NB = sorted_listB[-num_results:]\n",
        "top_NB = sorted(top_NB, reverse=True)\n",
        "# Quran\n",
        "cosine_scoresQ = util.pytorch_cos_sim(embedding1, embeddingQ)\n",
        "score_listQ = list(cosine_scoresQ[0])\n",
        "sorted_listQ = sorted(score_listQ)\n",
        "top_NQ = sorted_listQ[-num_results:]\n",
        "top_NQ = sorted(top_NQ, reverse=True)\n",
        "\n",
        "print(input_phrase)\n",
        "\n",
        "for i in range(len(top_NG)):\n",
        "  which_oneG = score_listG.index(top_NG[i])\n",
        "  print('Gita', top_NG[i], quoted_listG[which_oneG], df_Gita.iloc[which_oneG,0], df_Gita.iloc[which_oneG,1])\n",
        "\n",
        "for i in range(len(top_NB)):\n",
        "  which_oneB = score_listB.index(top_NB[i])\n",
        "  print('Bible', top_NB[i], quoted_listB[which_oneB], df_Bible.iloc[which_oneB,0], df_Bible.iloc[which_oneB,1], df_Bible.iloc[which_oneB,2])\n",
        "\n",
        "for i in range(len(top_NQ)):\n",
        "  which_oneQ = score_listQ.index(top_NQ[i])\n",
        "  print('Quran', top_NQ[i], quoted_listQ[which_oneQ], df_Quran.iloc[which_oneQ,0], df_Quran.iloc[which_oneQ,1])"
      ],
      "metadata": {
        "id": "_fVEMbvnNCkK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}